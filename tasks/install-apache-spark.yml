# (c) 2017 DataNexus Inc.  All Rights Reserved
---
# First, setup a couple of facts so that know which interface the UI should be bound
# to (based on whether or not we're securing the cluster) and so that we can test
# whether we're uploading the Spark distribution from a file local to the Ansible node
# or downloading the distribution from a URL
- set_fact:
    bind_iface: "{{(secure_cluster | default(false)) | ternary(data_iface, api_iface)}}"
    install_from_dir: "{{not(local_spark_file is undefined or local_spark_file is none or local_spark_file | trim == '')}}"
- set_fact:
    install_from_url: "{{not(spark_url is undefined or spark_url is none or spark_url | trim == '')}}"
  when: not(install_from_dir)
# set a fact containing the appropriate (private) IP addresses of the spark_master_nodes
# (if a list of spark_master_nodes was passed in)
- set_fact:
    spark_master_data_ips: "{{(spark_master_nodes | default([])) | map('extract', hostvars, ['ansible_' + data_iface, 'ipv4', 'address']) | list}}"
    spark_master_bind_ips: "{{(spark_master_nodes | default([])) | map('extract', hostvars, ['ansible_' + bind_iface, 'ipv4', 'address']) | list}}"
  when: spark_master_nodes != []
- set_fact:
    spark_master_data_ips: "{{spark_master_data_ips | default([data_addr])}}"
    bind_address: "{{hostvars[inventory_hostname]['ansible_' + bind_iface]['ipv4']['address']}}"
# If we're installing from a URL, then download the Spark
# distribution from that URL
- block:
  - name: Download Spark distribution to /tmp
    get_url:
      url: "{{spark_url}}"
      dest: /tmp
      mode: 0644
      validate_certs: no
    environment: "{{environment_vars}}"
  - set_fact:
      spark_filename: "{{spark_url | basename}}"
  when: install_from_url is defined and install_from_url
# If we're installing from a local file, then upload the Spark
# distribution from that local file
- block:
  - name: Upload Spark distribution from a local directory to /tmp
    copy:
      src: "{{local_spark_file}}"
      dest: "/tmp"
      mode: 0644
  - set_fact:
      spark_filename: "{{local_spark_file | basename}}"
  when: install_from_dir
# regardless of which option was used, we need to unpack the
# distribution file and setup the basic configuration options
# using a template
- block:
  # create the directory Spark will be unpacked into
  - name: Create Spark home directory {{spark_dir}}
    file:
      path: "{{spark_dir}}"
      state: directory
      mode: 0755
      owner: "{{spark_user}}"
      group: "{{spark_group}}"
  # unpack the distribution file
  - name: Unpack spark distribution into {{spark_dir}}
    unarchive:
      copy: no
      src: "/tmp/{{spark_filename}}"
      dest: "{{spark_dir}}"
      extra_opts: [ --strip-components=1 ]
      owner: "{{spark_user}}"
      group: "{{spark_group}}"
  # create the data directory
  - name: Create Spark data directory {{spark_data_dir}}/spark
    file:
      path: "{{spark_data_dir}}/spark"
      state: directory
      mode: 0755
      owner: "{{spark_user}}"
      group: "{{spark_group}}"
  become: true
